Tests
Hard attention vs Soft attention vs MORPH_SEGMENT
Dropout rates 0.3-0.7 for all three


Soft attention
Done:
    0.3
    0.4
    0.5
    0.6
    0.7

Submitted

Hard attention
Done:

Submitted:
    0.6
    0.7
    0.5
    0.4
    0.3

Hard large optimised at dropout: 0.6
Hard small optimised at dropout: 0.3
Transformer optimised at dropout: 0.3

Now to experiment with learning rates:
    the standard so far has been 1e-3 experiment 1e-4, 1e-5