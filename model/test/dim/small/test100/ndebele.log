INFO - 10/17/22 00:57:41 - 0:00:00 - command line argument: seed - 0
INFO - 10/17/22 00:57:41 - 0:00:00 - command line argument: train - ['data/ndebele/ndebele.train']
INFO - 10/17/22 00:57:41 - 0:00:00 - command line argument: dev - ['data/ndebele/ndebele.dev']
INFO - 10/17/22 00:57:41 - 0:00:00 - command line argument: test - ['data/ndebele/ndebele.test']
INFO - 10/17/22 00:57:41 - 0:00:00 - command line argument: model - 'model/test/hardattention/small/transformer0.2/ndebele'
INFO - 10/17/22 00:57:41 - 0:00:00 - command line argument: load - ''
INFO - 10/17/22 00:57:41 - 0:00:00 - command line argument: bs - 20
INFO - 10/17/22 00:57:41 - 0:00:00 - command line argument: epochs - 50
INFO - 10/17/22 00:57:41 - 0:00:00 - command line argument: max_steps - 0
INFO - 10/17/22 00:57:41 - 0:00:00 - command line argument: warmup_steps - 4000
INFO - 10/17/22 00:57:41 - 0:00:00 - command line argument: total_eval - -1
INFO - 10/17/22 00:57:41 - 0:00:00 - command line argument: optimizer - <Optimizer.adam: 'adam'>
INFO - 10/17/22 00:57:41 - 0:00:00 - command line argument: scheduler - <Scheduler.reducewhenstuck: 'reducewhenstuck'>
INFO - 10/17/22 00:57:41 - 0:00:00 - command line argument: lr - 0.001
INFO - 10/17/22 00:57:41 - 0:00:00 - command line argument: min_lr - 1e-05
INFO - 10/17/22 00:57:41 - 0:00:00 - command line argument: momentum - 0.9
INFO - 10/17/22 00:57:41 - 0:00:00 - command line argument: beta1 - 0.9
INFO - 10/17/22 00:57:41 - 0:00:00 - command line argument: beta2 - 0.999
INFO - 10/17/22 00:57:41 - 0:00:00 - command line argument: estop - 1e-08
INFO - 10/17/22 00:57:41 - 0:00:00 - command line argument: cooldown - 0
INFO - 10/17/22 00:57:41 - 0:00:00 - command line argument: patience - 0
INFO - 10/17/22 00:57:41 - 0:00:00 - command line argument: discount_factor - 0.5
INFO - 10/17/22 00:57:41 - 0:00:00 - command line argument: max_norm - 0
INFO - 10/17/22 00:57:41 - 0:00:00 - command line argument: gpuid - [0]
INFO - 10/17/22 00:57:41 - 0:00:00 - command line argument: loglevel - 'info'
INFO - 10/17/22 00:57:41 - 0:00:00 - command line argument: saveall - False
INFO - 10/17/22 00:57:41 - 0:00:00 - command line argument: shuffle - False
INFO - 10/17/22 00:57:41 - 0:00:00 - command line argument: cleanup_anyway - False
INFO - 10/17/22 00:57:41 - 0:00:00 - command line argument: dataset - <Data.g2p: 'g2p'>
INFO - 10/17/22 00:57:41 - 0:00:00 - command line argument: max_seq_len - 128
INFO - 10/17/22 00:57:41 - 0:00:00 - command line argument: max_decode_len - 128
INFO - 10/17/22 00:57:41 - 0:00:00 - command line argument: decode_beam_size - 5
INFO - 10/17/22 00:57:41 - 0:00:00 - command line argument: init - 'init/test/large/seed-0/ndebele'
INFO - 10/17/22 00:57:41 - 0:00:00 - command line argument: dropout - 0.2
INFO - 10/17/22 00:57:41 - 0:00:00 - command line argument: embed_dim - 100
INFO - 10/17/22 00:57:41 - 0:00:00 - command line argument: nb_heads - 4
INFO - 10/17/22 00:57:41 - 0:00:00 - command line argument: src_layer - 1
INFO - 10/17/22 00:57:41 - 0:00:00 - command line argument: trg_layer - 1
INFO - 10/17/22 00:57:41 - 0:00:00 - command line argument: src_hs - 200
INFO - 10/17/22 00:57:41 - 0:00:00 - command line argument: trg_hs - 200
INFO - 10/17/22 00:57:41 - 0:00:00 - command line argument: label_smooth - 0.0
INFO - 10/17/22 00:57:41 - 0:00:00 - command line argument: tie_trg_embed - False
INFO - 10/17/22 00:57:41 - 0:00:00 - command line argument: arch - <Arch.hard: 'hard'>
INFO - 10/17/22 00:57:41 - 0:00:00 - command line argument: nb_sample - 2
INFO - 10/17/22 00:57:41 - 0:00:00 - command line argument: wid_siz - 11
INFO - 10/17/22 00:57:41 - 0:00:00 - command line argument: indtag - False
INFO - 10/17/22 00:57:41 - 0:00:00 - command line argument: decode - <Decode.greedy: 'greedy'>
INFO - 10/17/22 00:57:41 - 0:00:00 - command line argument: mono - False
INFO - 10/17/22 00:57:41 - 0:00:00 - command line argument: bestacc - False
INFO - 10/17/22 00:57:41 - 0:00:00 - src vocab size 68
INFO - 10/17/22 00:57:41 - 0:00:00 - trg vocab size 66
INFO - 10/17/22 00:57:41 - 0:00:00 - src vocab ['<PAD>', '<BOS>', '<EOS>', '<UNK>', '%', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']
INFO - 10/17/22 00:57:41 - 0:00:00 - trg vocab ['<PAD>', '<BOS>', '<EOS>', '<UNK>', '%', "'", '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'V', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']
INFO - 10/17/22 00:57:41 - 0:00:00 - model: HardAttnTransducer(
                                       (src_embed): Embedding(68, 100, padding_idx=0)
                                       (trg_embed): Embedding(66, 100, padding_idx=0)
                                       (enc_rnn): LSTM(100, 200, dropout=0.2, bidirectional=True)
                                       (dec_rnn): StackedLSTM(
                                         (layers): ModuleList(
                                           (0): LSTMCell(100, 200)
                                         )
                                         (dropout): Dropout(p=0.2, inplace=False)
                                       )
                                       (scale_enc_hs): Linear(in_features=400, out_features=200, bias=True)
                                       (attn): Attention()
                                       (linear_out): Linear(in_features=600, out_features=600, bias=True)
                                       (final_out): Linear(in_features=600, out_features=66, bias=True)
                                       (dropout): Dropout(p=0.2, inplace=False)
                                     )
INFO - 10/17/22 00:57:41 - 0:00:00 - number of parameter 1218666
INFO - 10/17/22 02:25:24 - 0:00:00 - command line argument: seed - 0
INFO - 10/17/22 02:25:24 - 0:00:00 - command line argument: train - ['data/ndebele/ndebele.train']
INFO - 10/17/22 02:25:24 - 0:00:00 - command line argument: dev - ['data/ndebele/ndebele.dev']
INFO - 10/17/22 02:25:24 - 0:00:00 - command line argument: test - ['data/ndebele/ndebele.test']
INFO - 10/17/22 02:25:24 - 0:00:00 - command line argument: model - 'model/test/hardattention/small/transformer0.2/ndebele'
INFO - 10/17/22 02:25:24 - 0:00:00 - command line argument: load - ''
INFO - 10/17/22 02:25:24 - 0:00:00 - command line argument: bs - 20
INFO - 10/17/22 02:25:24 - 0:00:00 - command line argument: epochs - 50
INFO - 10/17/22 02:25:24 - 0:00:00 - command line argument: max_steps - 0
INFO - 10/17/22 02:25:24 - 0:00:00 - command line argument: warmup_steps - 4000
INFO - 10/17/22 02:25:24 - 0:00:00 - command line argument: total_eval - -1
INFO - 10/17/22 02:25:24 - 0:00:00 - command line argument: optimizer - <Optimizer.adam: 'adam'>
INFO - 10/17/22 02:25:24 - 0:00:00 - command line argument: scheduler - <Scheduler.reducewhenstuck: 'reducewhenstuck'>
INFO - 10/17/22 02:25:24 - 0:00:00 - command line argument: lr - 0.001
INFO - 10/17/22 02:25:24 - 0:00:00 - command line argument: min_lr - 1e-05
INFO - 10/17/22 02:25:24 - 0:00:00 - command line argument: momentum - 0.9
INFO - 10/17/22 02:25:24 - 0:00:00 - command line argument: beta1 - 0.9
INFO - 10/17/22 02:25:24 - 0:00:00 - command line argument: beta2 - 0.999
INFO - 10/17/22 02:25:24 - 0:00:00 - command line argument: estop - 1e-08
INFO - 10/17/22 02:25:24 - 0:00:00 - command line argument: cooldown - 0
INFO - 10/17/22 02:25:24 - 0:00:00 - command line argument: patience - 0
INFO - 10/17/22 02:25:24 - 0:00:00 - command line argument: discount_factor - 0.5
INFO - 10/17/22 02:25:24 - 0:00:00 - command line argument: max_norm - 0
INFO - 10/17/22 02:25:24 - 0:00:00 - command line argument: gpuid - [0]
INFO - 10/17/22 02:25:24 - 0:00:00 - command line argument: loglevel - 'info'
INFO - 10/17/22 02:25:24 - 0:00:00 - command line argument: saveall - False
INFO - 10/17/22 02:25:24 - 0:00:00 - command line argument: shuffle - False
INFO - 10/17/22 02:25:24 - 0:00:00 - command line argument: cleanup_anyway - False
INFO - 10/17/22 02:25:24 - 0:00:00 - command line argument: dataset - <Data.g2p: 'g2p'>
INFO - 10/17/22 02:25:24 - 0:00:00 - command line argument: max_seq_len - 128
INFO - 10/17/22 02:25:24 - 0:00:00 - command line argument: max_decode_len - 128
INFO - 10/17/22 02:25:24 - 0:00:00 - command line argument: decode_beam_size - 5
INFO - 10/17/22 02:25:24 - 0:00:00 - command line argument: init - 'init/test/large/seed-0/ndebele'
INFO - 10/17/22 02:25:24 - 0:00:00 - command line argument: dropout - 0.2
INFO - 10/17/22 02:25:24 - 0:00:00 - command line argument: embed_dim - 100
INFO - 10/17/22 02:25:24 - 0:00:00 - command line argument: nb_heads - 4
INFO - 10/17/22 02:25:24 - 0:00:00 - command line argument: src_layer - 1
INFO - 10/17/22 02:25:24 - 0:00:00 - command line argument: trg_layer - 1
INFO - 10/17/22 02:25:24 - 0:00:00 - command line argument: src_hs - 200
INFO - 10/17/22 02:25:24 - 0:00:00 - command line argument: trg_hs - 200
INFO - 10/17/22 02:25:24 - 0:00:00 - command line argument: label_smooth - 0.0
INFO - 10/17/22 02:25:24 - 0:00:00 - command line argument: tie_trg_embed - False
INFO - 10/17/22 02:25:24 - 0:00:00 - command line argument: arch - <Arch.hard: 'hard'>
INFO - 10/17/22 02:25:24 - 0:00:00 - command line argument: nb_sample - 2
INFO - 10/17/22 02:25:24 - 0:00:00 - command line argument: wid_siz - 11
INFO - 10/17/22 02:25:24 - 0:00:00 - command line argument: indtag - False
INFO - 10/17/22 02:25:24 - 0:00:00 - command line argument: decode - <Decode.greedy: 'greedy'>
INFO - 10/17/22 02:25:24 - 0:00:00 - command line argument: mono - False
INFO - 10/17/22 02:25:24 - 0:00:00 - command line argument: bestacc - False
INFO - 10/17/22 02:25:24 - 0:00:00 - src vocab size 68
INFO - 10/17/22 02:25:24 - 0:00:00 - trg vocab size 66
INFO - 10/17/22 02:25:24 - 0:00:00 - src vocab ['<PAD>', '<BOS>', '<EOS>', '<UNK>', '%', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']
INFO - 10/17/22 02:25:24 - 0:00:00 - trg vocab ['<PAD>', '<BOS>', '<EOS>', '<UNK>', '%', "'", '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'V', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']
INFO - 10/17/22 02:25:24 - 0:00:00 - model: HardAttnTransducer(
                                       (src_embed): Embedding(68, 100, padding_idx=0)
                                       (trg_embed): Embedding(66, 100, padding_idx=0)
                                       (enc_rnn): LSTM(100, 200, dropout=0.2, bidirectional=True)
                                       (dec_rnn): StackedLSTM(
                                         (layers): ModuleList(
                                           (0): LSTMCell(100, 200)
                                         )
                                         (dropout): Dropout(p=0.2, inplace=False)
                                       )
                                       (scale_enc_hs): Linear(in_features=400, out_features=200, bias=True)
                                       (attn): Attention()
                                       (linear_out): Linear(in_features=600, out_features=600, bias=True)
                                       (final_out): Linear(in_features=600, out_features=66, bias=True)
                                       (dropout): Dropout(p=0.2, inplace=False)
                                     )
INFO - 10/17/22 02:25:24 - 0:00:00 - number of parameter 1218666
INFO - 10/17/22 02:32:45 - 0:00:00 - command line argument: seed - 0
INFO - 10/17/22 02:32:45 - 0:00:00 - command line argument: train - ['data/ndebele/ndebele.train']
INFO - 10/17/22 02:32:45 - 0:00:00 - command line argument: dev - ['data/ndebele/ndebele.dev']
INFO - 10/17/22 02:32:45 - 0:00:00 - command line argument: test - ['data/ndebele/ndebele.test']
INFO - 10/17/22 02:32:45 - 0:00:00 - command line argument: model - 'model/test/hardattention/small/transformer0.2/ndebele'
INFO - 10/17/22 02:32:45 - 0:00:00 - command line argument: load - ''
INFO - 10/17/22 02:32:45 - 0:00:00 - command line argument: bs - 20
INFO - 10/17/22 02:32:45 - 0:00:00 - command line argument: epochs - 50
INFO - 10/17/22 02:32:45 - 0:00:00 - command line argument: max_steps - 0
INFO - 10/17/22 02:32:45 - 0:00:00 - command line argument: warmup_steps - 4000
INFO - 10/17/22 02:32:45 - 0:00:00 - command line argument: total_eval - -1
INFO - 10/17/22 02:32:45 - 0:00:00 - command line argument: optimizer - <Optimizer.adam: 'adam'>
INFO - 10/17/22 02:32:45 - 0:00:00 - command line argument: scheduler - <Scheduler.reducewhenstuck: 'reducewhenstuck'>
INFO - 10/17/22 02:32:45 - 0:00:00 - command line argument: lr - 0.001
INFO - 10/17/22 02:32:45 - 0:00:00 - command line argument: min_lr - 1e-05
INFO - 10/17/22 02:32:45 - 0:00:00 - command line argument: momentum - 0.9
INFO - 10/17/22 02:32:45 - 0:00:00 - command line argument: beta1 - 0.9
INFO - 10/17/22 02:32:45 - 0:00:00 - command line argument: beta2 - 0.999
INFO - 10/17/22 02:32:45 - 0:00:00 - command line argument: estop - 1e-08
INFO - 10/17/22 02:32:45 - 0:00:00 - command line argument: cooldown - 0
INFO - 10/17/22 02:32:45 - 0:00:00 - command line argument: patience - 0
INFO - 10/17/22 02:32:45 - 0:00:00 - command line argument: discount_factor - 0.5
INFO - 10/17/22 02:32:45 - 0:00:00 - command line argument: max_norm - 0
INFO - 10/17/22 02:32:45 - 0:00:00 - command line argument: gpuid - [0]
INFO - 10/17/22 02:32:45 - 0:00:00 - command line argument: loglevel - 'info'
INFO - 10/17/22 02:32:45 - 0:00:00 - command line argument: saveall - False
INFO - 10/17/22 02:32:45 - 0:00:00 - command line argument: shuffle - False
INFO - 10/17/22 02:32:45 - 0:00:00 - command line argument: cleanup_anyway - False
INFO - 10/17/22 02:32:45 - 0:00:00 - command line argument: dataset - <Data.g2p: 'g2p'>
INFO - 10/17/22 02:32:45 - 0:00:00 - command line argument: max_seq_len - 128
INFO - 10/17/22 02:32:45 - 0:00:00 - command line argument: max_decode_len - 128
INFO - 10/17/22 02:32:45 - 0:00:00 - command line argument: decode_beam_size - 5
INFO - 10/17/22 02:32:45 - 0:00:00 - command line argument: init - 'init/test/large/seed-0/ndebele/small'
INFO - 10/17/22 02:32:45 - 0:00:00 - command line argument: dropout - 0.2
INFO - 10/17/22 02:32:45 - 0:00:00 - command line argument: embed_dim - 100
INFO - 10/17/22 02:32:45 - 0:00:00 - command line argument: nb_heads - 4
INFO - 10/17/22 02:32:45 - 0:00:00 - command line argument: src_layer - 1
INFO - 10/17/22 02:32:45 - 0:00:00 - command line argument: trg_layer - 1
INFO - 10/17/22 02:32:45 - 0:00:00 - command line argument: src_hs - 200
INFO - 10/17/22 02:32:45 - 0:00:00 - command line argument: trg_hs - 200
INFO - 10/17/22 02:32:45 - 0:00:00 - command line argument: label_smooth - 0.0
INFO - 10/17/22 02:32:45 - 0:00:00 - command line argument: tie_trg_embed - False
INFO - 10/17/22 02:32:45 - 0:00:00 - command line argument: arch - <Arch.hard: 'hard'>
INFO - 10/17/22 02:32:45 - 0:00:00 - command line argument: nb_sample - 2
INFO - 10/17/22 02:32:45 - 0:00:00 - command line argument: wid_siz - 11
INFO - 10/17/22 02:32:45 - 0:00:00 - command line argument: indtag - False
INFO - 10/17/22 02:32:45 - 0:00:00 - command line argument: decode - <Decode.greedy: 'greedy'>
INFO - 10/17/22 02:32:45 - 0:00:00 - command line argument: mono - False
INFO - 10/17/22 02:32:45 - 0:00:00 - command line argument: bestacc - False
INFO - 10/17/22 02:32:45 - 0:00:00 - src vocab size 68
INFO - 10/17/22 02:32:45 - 0:00:00 - trg vocab size 66
INFO - 10/17/22 02:32:45 - 0:00:00 - src vocab ['<PAD>', '<BOS>', '<EOS>', '<UNK>', '%', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']
INFO - 10/17/22 02:32:45 - 0:00:00 - trg vocab ['<PAD>', '<BOS>', '<EOS>', '<UNK>', '%', "'", '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'V', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']
INFO - 10/17/22 02:32:45 - 0:00:00 - model: HardAttnTransducer(
                                       (src_embed): Embedding(68, 100, padding_idx=0)
                                       (trg_embed): Embedding(66, 100, padding_idx=0)
                                       (enc_rnn): LSTM(100, 200, dropout=0.2, bidirectional=True)
                                       (dec_rnn): StackedLSTM(
                                         (layers): ModuleList(
                                           (0): LSTMCell(100, 200)
                                         )
                                         (dropout): Dropout(p=0.2, inplace=False)
                                       )
                                       (scale_enc_hs): Linear(in_features=400, out_features=200, bias=True)
                                       (attn): Attention()
                                       (linear_out): Linear(in_features=600, out_features=600, bias=True)
                                       (final_out): Linear(in_features=600, out_features=66, bias=True)
                                       (dropout): Dropout(p=0.2, inplace=False)
                                     )
INFO - 10/17/22 02:32:45 - 0:00:00 - number of parameter 1218666
INFO - 10/17/22 06:54:29 - 0:00:00 - command line argument: seed - 0
INFO - 10/17/22 06:54:29 - 0:00:00 - command line argument: train - ['data/ndebele/ndebele.train']
INFO - 10/17/22 06:54:29 - 0:00:00 - command line argument: dev - ['data/ndebele/ndebele.dev']
INFO - 10/17/22 06:54:29 - 0:00:00 - command line argument: test - ['data/ndebele/ndebele.test']
INFO - 10/17/22 06:54:29 - 0:00:00 - command line argument: model - 'model/test/hardattention/small/transformer0.2/ndebele'
INFO - 10/17/22 06:54:29 - 0:00:00 - command line argument: load - ''
INFO - 10/17/22 06:54:29 - 0:00:00 - command line argument: bs - 20
INFO - 10/17/22 06:54:29 - 0:00:00 - command line argument: epochs - 50
INFO - 10/17/22 06:54:29 - 0:00:00 - command line argument: max_steps - 0
INFO - 10/17/22 06:54:29 - 0:00:00 - command line argument: warmup_steps - 4000
INFO - 10/17/22 06:54:29 - 0:00:00 - command line argument: total_eval - -1
INFO - 10/17/22 06:54:29 - 0:00:00 - command line argument: optimizer - <Optimizer.adam: 'adam'>
INFO - 10/17/22 06:54:29 - 0:00:00 - command line argument: scheduler - <Scheduler.reducewhenstuck: 'reducewhenstuck'>
INFO - 10/17/22 06:54:29 - 0:00:00 - command line argument: lr - 0.001
INFO - 10/17/22 06:54:29 - 0:00:00 - command line argument: min_lr - 1e-05
INFO - 10/17/22 06:54:29 - 0:00:00 - command line argument: momentum - 0.9
INFO - 10/17/22 06:54:29 - 0:00:00 - command line argument: beta1 - 0.9
INFO - 10/17/22 06:54:29 - 0:00:00 - command line argument: beta2 - 0.999
INFO - 10/17/22 06:54:29 - 0:00:00 - command line argument: estop - 1e-08
INFO - 10/17/22 06:54:29 - 0:00:00 - command line argument: cooldown - 0
INFO - 10/17/22 06:54:29 - 0:00:00 - command line argument: patience - 0
INFO - 10/17/22 06:54:29 - 0:00:00 - command line argument: discount_factor - 0.5
INFO - 10/17/22 06:54:29 - 0:00:00 - command line argument: max_norm - 0
INFO - 10/17/22 06:54:29 - 0:00:00 - command line argument: gpuid - [0]
INFO - 10/17/22 06:54:29 - 0:00:00 - command line argument: loglevel - 'info'
INFO - 10/17/22 06:54:29 - 0:00:00 - command line argument: saveall - False
INFO - 10/17/22 06:54:29 - 0:00:00 - command line argument: shuffle - False
INFO - 10/17/22 06:54:29 - 0:00:00 - command line argument: cleanup_anyway - False
INFO - 10/17/22 06:54:29 - 0:00:00 - command line argument: dataset - <Data.g2p: 'g2p'>
INFO - 10/17/22 06:54:29 - 0:00:00 - command line argument: max_seq_len - 128
INFO - 10/17/22 06:54:29 - 0:00:00 - command line argument: max_decode_len - 128
INFO - 10/17/22 06:54:29 - 0:00:00 - command line argument: decode_beam_size - 5
INFO - 10/17/22 06:54:29 - 0:00:00 - command line argument: init - ''
INFO - 10/17/22 06:54:29 - 0:00:00 - command line argument: dropout - 0.2
INFO - 10/17/22 06:54:29 - 0:00:00 - command line argument: embed_dim - 100
INFO - 10/17/22 06:54:29 - 0:00:00 - command line argument: nb_heads - 4
INFO - 10/17/22 06:54:29 - 0:00:00 - command line argument: src_layer - 1
INFO - 10/17/22 06:54:29 - 0:00:00 - command line argument: trg_layer - 1
INFO - 10/17/22 06:54:29 - 0:00:00 - command line argument: src_hs - 200
INFO - 10/17/22 06:54:29 - 0:00:00 - command line argument: trg_hs - 200
INFO - 10/17/22 06:54:29 - 0:00:00 - command line argument: label_smooth - 0.0
INFO - 10/17/22 06:54:29 - 0:00:00 - command line argument: tie_trg_embed - False
INFO - 10/17/22 06:54:29 - 0:00:00 - command line argument: arch - <Arch.hard: 'hard'>
INFO - 10/17/22 06:54:29 - 0:00:00 - command line argument: nb_sample - 2
INFO - 10/17/22 06:54:29 - 0:00:00 - command line argument: wid_siz - 11
INFO - 10/17/22 06:54:29 - 0:00:00 - command line argument: indtag - False
INFO - 10/17/22 06:54:29 - 0:00:00 - command line argument: decode - <Decode.greedy: 'greedy'>
INFO - 10/17/22 06:54:29 - 0:00:00 - command line argument: mono - False
INFO - 10/17/22 06:54:29 - 0:00:00 - command line argument: bestacc - False
INFO - 10/17/22 06:54:29 - 0:00:00 - src vocab size 68
INFO - 10/17/22 06:54:29 - 0:00:00 - trg vocab size 66
INFO - 10/17/22 06:54:29 - 0:00:00 - src vocab ['<PAD>', '<BOS>', '<EOS>', '<UNK>', '%', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']
INFO - 10/17/22 06:54:29 - 0:00:00 - trg vocab ['<PAD>', '<BOS>', '<EOS>', '<UNK>', '%', "'", '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'V', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']
INFO - 10/17/22 06:54:29 - 0:00:00 - model: HardAttnTransducer(
                                       (src_embed): Embedding(68, 100, padding_idx=0)
                                       (trg_embed): Embedding(66, 100, padding_idx=0)
                                       (enc_rnn): LSTM(100, 200, dropout=0.2, bidirectional=True)
                                       (dec_rnn): StackedLSTM(
                                         (layers): ModuleList(
                                           (0): LSTMCell(100, 200)
                                         )
                                         (dropout): Dropout(p=0.2, inplace=False)
                                       )
                                       (scale_enc_hs): Linear(in_features=400, out_features=200, bias=True)
                                       (attn): Attention()
                                       (linear_out): Linear(in_features=600, out_features=600, bias=True)
                                       (final_out): Linear(in_features=600, out_features=66, bias=True)
                                       (dropout): Dropout(p=0.2, inplace=False)
                                     )
INFO - 10/17/22 06:54:29 - 0:00:00 - number of parameter 1218666
INFO - 10/17/22 06:54:32 - 0:00:03 - maximum training 32350 steps (50 epochs)
INFO - 10/17/22 06:54:32 - 0:00:03 - evaluate every 1 epochs
INFO - 10/17/22 06:54:32 - 0:00:03 - At 0-th epoch with lr 0.001000.
INFO - 10/17/22 06:54:52 - 0:00:22 - Running average train loss is 0.48320518265920226 at epoch 0
INFO - 10/17/22 06:54:52 - 0:00:22 - At 1-th epoch with lr 0.001000.
INFO - 10/17/22 06:55:09 - 0:00:40 - Running average train loss is 0.15270182330282248 at epoch 1
INFO - 10/17/22 06:55:10 - 0:00:41 - Average dev loss is 0.1340037321815124 at epoch 1
INFO - 10/17/22 06:55:11 - 0:00:42 - dev accuracy is 54.3165 at epoch 1
INFO - 10/17/22 06:55:11 - 0:00:42 - dev phenome error rate is 0.1047 at epoch 1
INFO - 10/17/22 06:55:11 - 0:00:42 - At 2-th epoch with lr 0.001000.
INFO - 10/17/22 06:55:28 - 0:00:59 - Running average train loss is 0.11055009093832288 at epoch 2
INFO - 10/17/22 06:55:29 - 0:01:00 - Average dev loss is 0.09430491239405596 at epoch 2
INFO - 10/17/22 06:55:30 - 0:01:01 - dev accuracy is 66.0072 at epoch 2
INFO - 10/17/22 06:55:30 - 0:01:01 - dev phenome error rate is 0.0726 at epoch 2
INFO - 10/17/22 06:55:30 - 0:01:01 - At 3-th epoch with lr 0.001000.
INFO - 10/17/22 06:55:48 - 0:01:18 - Running average train loss is 0.09036290440190198 at epoch 3
INFO - 10/17/22 06:55:48 - 0:01:19 - Average dev loss is 0.07975960454115501 at epoch 3
INFO - 10/17/22 06:55:49 - 0:01:20 - dev accuracy is 69.7842 at epoch 3
INFO - 10/17/22 06:55:49 - 0:01:20 - dev phenome error rate is 0.0566 at epoch 3
INFO - 10/17/22 06:55:49 - 0:01:20 - At 4-th epoch with lr 0.001000.
INFO - 10/17/22 06:56:07 - 0:01:37 - Running average train loss is 0.0775636872010366 at epoch 4
INFO - 10/17/22 06:56:07 - 0:01:38 - Average dev loss is 0.06567001179433786 at epoch 4
INFO - 10/17/22 06:56:08 - 0:01:39 - dev accuracy is 73.1115 at epoch 4
INFO - 10/17/22 06:56:08 - 0:01:39 - dev phenome error rate is 0.0456 at epoch 4
INFO - 10/17/22 06:56:08 - 0:01:39 - At 5-th epoch with lr 0.001000.
INFO - 10/17/22 06:56:26 - 0:01:56 - Running average train loss is 0.06459638421960648 at epoch 5
INFO - 10/17/22 06:56:26 - 0:01:57 - Average dev loss is 0.06101301098958804 at epoch 5
INFO - 10/17/22 06:56:27 - 0:01:58 - dev accuracy is 75.3597 at epoch 5
INFO - 10/17/22 06:56:27 - 0:01:58 - dev phenome error rate is 0.0362 at epoch 5
INFO - 10/17/22 06:56:27 - 0:01:58 - At 6-th epoch with lr 0.001000.
INFO - 10/17/22 06:56:45 - 0:02:15 - Running average train loss is 0.058683356794872694 at epoch 6
INFO - 10/17/22 06:56:45 - 0:02:16 - Average dev loss is 0.05606130206814179 at epoch 6
INFO - 10/17/22 06:56:46 - 0:02:17 - dev accuracy is 77.3381 at epoch 6
INFO - 10/17/22 06:56:46 - 0:02:17 - dev phenome error rate is 0.0347 at epoch 6
INFO - 10/17/22 06:56:46 - 0:02:17 - At 7-th epoch with lr 0.001000.
INFO - 10/17/22 06:57:03 - 0:02:34 - Running average train loss is 0.053017654902426964 at epoch 7
INFO - 10/17/22 06:57:04 - 0:02:35 - Average dev loss is 0.05267950797166962 at epoch 7
INFO - 10/17/22 06:57:05 - 0:02:36 - dev accuracy is 79.3165 at epoch 7
INFO - 10/17/22 06:57:05 - 0:02:36 - dev phenome error rate is 0.0307 at epoch 7
INFO - 10/17/22 06:57:05 - 0:02:36 - At 8-th epoch with lr 0.001000.
INFO - 10/17/22 06:57:22 - 0:02:53 - Running average train loss is 0.046331904676686714 at epoch 8
INFO - 10/17/22 06:57:23 - 0:02:54 - Average dev loss is 0.048800481827213214 at epoch 8
INFO - 10/17/22 06:57:24 - 0:02:55 - dev accuracy is 80.7554 at epoch 8
INFO - 10/17/22 06:57:24 - 0:02:55 - dev phenome error rate is 0.0269 at epoch 8
INFO - 10/17/22 06:57:24 - 0:02:55 - At 9-th epoch with lr 0.001000.
INFO - 10/17/22 06:57:41 - 0:03:12 - Running average train loss is 0.04327053940888074 at epoch 9
INFO - 10/17/22 06:57:42 - 0:03:13 - Average dev loss is 0.05268259309232235 at epoch 9
INFO - 10/17/22 06:57:43 - 0:03:14 - dev accuracy is 81.295 at epoch 9
INFO - 10/17/22 06:57:43 - 0:03:14 - dev phenome error rate is 0.027 at epoch 9
INFO - 10/17/22 06:57:43 - 0:03:14 - At 10-th epoch with lr 0.000500.
INFO - 10/17/22 06:58:00 - 0:03:31 - Running average train loss is 0.03162740149471903 at epoch 10
INFO - 10/17/22 06:58:01 - 0:03:32 - Average dev loss is 0.034219538463422884 at epoch 10
INFO - 10/17/22 06:58:02 - 0:03:33 - dev accuracy is 85.7014 at epoch 10
INFO - 10/17/22 06:58:02 - 0:03:33 - dev phenome error rate is 0.0174 at epoch 10
INFO - 10/17/22 06:58:02 - 0:03:33 - At 11-th epoch with lr 0.000500.
INFO - 10/17/22 06:58:19 - 0:03:50 - Running average train loss is 0.0246581858329164 at epoch 11
INFO - 10/17/22 06:58:20 - 0:03:51 - Average dev loss is 0.030463922342572076 at epoch 11
INFO - 10/17/22 06:58:21 - 0:03:52 - dev accuracy is 87.2302 at epoch 11
INFO - 10/17/22 06:58:21 - 0:03:52 - dev phenome error rate is 0.0157 at epoch 11
INFO - 10/17/22 06:58:21 - 0:03:52 - At 12-th epoch with lr 0.000500.
INFO - 10/17/22 06:58:38 - 0:04:09 - Running average train loss is 0.022643334956312842 at epoch 12
INFO - 10/17/22 06:58:39 - 0:04:09 - Average dev loss is 0.03157542811420101 at epoch 12
INFO - 10/17/22 06:58:40 - 0:04:10 - dev accuracy is 87.5899 at epoch 12
INFO - 10/17/22 06:58:40 - 0:04:10 - dev phenome error rate is 0.0147 at epoch 12
INFO - 10/17/22 06:58:40 - 0:04:10 - At 13-th epoch with lr 0.000250.
INFO - 10/17/22 06:58:57 - 0:04:28 - Running average train loss is 0.019512314841883916 at epoch 13
INFO - 10/17/22 06:58:58 - 0:04:28 - Average dev loss is 0.023371964213080132 at epoch 13
INFO - 10/17/22 06:58:59 - 0:04:29 - dev accuracy is 88.759 at epoch 13
INFO - 10/17/22 06:58:59 - 0:04:29 - dev phenome error rate is 0.0134 at epoch 13
INFO - 10/17/22 06:58:59 - 0:04:29 - At 14-th epoch with lr 0.000250.
INFO - 10/17/22 06:59:16 - 0:04:47 - Running average train loss is 0.017053765399522584 at epoch 14
INFO - 10/17/22 06:59:17 - 0:04:47 - Average dev loss is 0.024360178163848244 at epoch 14
INFO - 10/17/22 06:59:18 - 0:04:48 - dev accuracy is 89.0288 at epoch 14
INFO - 10/17/22 06:59:18 - 0:04:48 - dev phenome error rate is 0.0126 at epoch 14
INFO - 10/17/22 06:59:18 - 0:04:48 - At 15-th epoch with lr 0.000125.
INFO - 10/17/22 06:59:35 - 0:05:06 - Running average train loss is 0.015611469290050832 at epoch 15
INFO - 10/17/22 06:59:36 - 0:05:06 - Average dev loss is 0.020455265946041506 at epoch 15
INFO - 10/17/22 06:59:37 - 0:05:07 - dev accuracy is 90.3777 at epoch 15
INFO - 10/17/22 06:59:37 - 0:05:07 - dev phenome error rate is 0.0113 at epoch 15
INFO - 10/17/22 06:59:37 - 0:05:07 - At 16-th epoch with lr 0.000125.
INFO - 10/17/22 06:59:54 - 0:05:25 - Running average train loss is 0.014502885994602893 at epoch 16
INFO - 10/17/22 06:59:55 - 0:05:25 - Average dev loss is 0.02071924564571908 at epoch 16
INFO - 10/17/22 06:59:56 - 0:05:26 - dev accuracy is 90.3777 at epoch 16
INFO - 10/17/22 06:59:56 - 0:05:26 - dev phenome error rate is 0.0108 at epoch 16
INFO - 10/17/22 06:59:56 - 0:05:26 - At 17-th epoch with lr 0.000063.
INFO - 10/17/22 07:00:13 - 0:05:44 - Running average train loss is 0.014457595532463497 at epoch 17
INFO - 10/17/22 07:00:14 - 0:05:44 - Average dev loss is 0.018546072916629224 at epoch 17
INFO - 10/17/22 07:00:15 - 0:05:45 - dev accuracy is 90.7374 at epoch 17
INFO - 10/17/22 07:00:15 - 0:05:45 - dev phenome error rate is 0.0098 at epoch 17
INFO - 10/17/22 07:00:15 - 0:05:45 - At 18-th epoch with lr 0.000063.
INFO - 10/17/22 07:00:32 - 0:06:03 - Running average train loss is 0.013553638772145087 at epoch 18
INFO - 10/17/22 07:00:33 - 0:06:03 - Average dev loss is 0.018291380462379984 at epoch 18
INFO - 10/17/22 07:00:34 - 0:06:04 - dev accuracy is 90.8273 at epoch 18
INFO - 10/17/22 07:00:34 - 0:06:04 - dev phenome error rate is 0.0104 at epoch 18
INFO - 10/17/22 07:00:34 - 0:06:04 - At 19-th epoch with lr 0.000063.
INFO - 10/17/22 07:00:51 - 0:06:22 - Running average train loss is 0.013222202582992297 at epoch 19
INFO - 10/17/22 07:00:52 - 0:06:22 - Average dev loss is 0.01812771906485208 at epoch 19
INFO - 10/17/22 07:00:53 - 0:06:23 - dev accuracy is 90.8273 at epoch 19
INFO - 10/17/22 07:00:53 - 0:06:23 - dev phenome error rate is 0.0104 at epoch 19
INFO - 10/17/22 07:00:53 - 0:06:23 - At 20-th epoch with lr 0.000063.
INFO - 10/17/22 07:01:10 - 0:06:41 - Running average train loss is 0.013117093915048514 at epoch 20
INFO - 10/17/22 07:01:11 - 0:06:41 - Average dev loss is 0.017991762305609883 at epoch 20
INFO - 10/17/22 07:01:12 - 0:06:42 - dev accuracy is 91.0072 at epoch 20
INFO - 10/17/22 07:01:12 - 0:06:42 - dev phenome error rate is 0.01 at epoch 20
INFO - 10/17/22 07:01:12 - 0:06:42 - At 21-th epoch with lr 0.000063.
INFO - 10/17/22 07:01:29 - 0:07:00 - Running average train loss is 0.012862489095704062 at epoch 21
INFO - 10/17/22 07:01:30 - 0:07:00 - Average dev loss is 0.018216470409578717 at epoch 21
INFO - 10/17/22 07:01:31 - 0:07:01 - dev accuracy is 91.0072 at epoch 21
INFO - 10/17/22 07:01:31 - 0:07:01 - dev phenome error rate is 0.0104 at epoch 21
INFO - 10/17/22 07:01:31 - 0:07:01 - At 22-th epoch with lr 0.000031.
INFO - 10/17/22 07:01:48 - 0:07:19 - Running average train loss is 0.013307736559761646 at epoch 22
INFO - 10/17/22 07:01:49 - 0:07:19 - Average dev loss is 0.016476238694471807 at epoch 22
INFO - 10/17/22 07:01:50 - 0:07:20 - dev accuracy is 91.3669 at epoch 22
INFO - 10/17/22 07:01:50 - 0:07:20 - dev phenome error rate is 0.0102 at epoch 22
INFO - 10/17/22 07:01:50 - 0:07:20 - At 23-th epoch with lr 0.000031.
INFO - 10/17/22 07:02:07 - 0:07:38 - Running average train loss is 0.012569390479698375 at epoch 23
INFO - 10/17/22 07:02:08 - 0:07:38 - Average dev loss is 0.01617172545162388 at epoch 23
INFO - 10/17/22 07:02:09 - 0:07:39 - dev accuracy is 91.277 at epoch 23
INFO - 10/17/22 07:02:09 - 0:07:39 - dev phenome error rate is 0.0103 at epoch 23
INFO - 10/17/22 07:02:09 - 0:07:39 - At 24-th epoch with lr 0.000031.
INFO - 10/17/22 07:02:26 - 0:07:57 - Running average train loss is 0.012709278533259663 at epoch 24
INFO - 10/17/22 07:02:27 - 0:07:57 - Average dev loss is 0.0159374779699227 at epoch 24
INFO - 10/17/22 07:02:28 - 0:07:58 - dev accuracy is 91.8165 at epoch 24
INFO - 10/17/22 07:02:28 - 0:07:58 - dev phenome error rate is 0.0096 at epoch 24
INFO - 10/17/22 07:02:28 - 0:07:58 - At 25-th epoch with lr 0.000031.
INFO - 10/17/22 07:02:45 - 0:08:16 - Running average train loss is 0.012680168931157986 at epoch 25
INFO - 10/17/22 07:02:46 - 0:08:16 - Average dev loss is 0.015660942792713354 at epoch 25
INFO - 10/17/22 07:02:47 - 0:08:17 - dev accuracy is 91.5468 at epoch 25
INFO - 10/17/22 07:02:47 - 0:08:17 - dev phenome error rate is 0.0098 at epoch 25
INFO - 10/17/22 07:02:47 - 0:08:17 - At 26-th epoch with lr 0.000031.
INFO - 10/17/22 07:03:04 - 0:08:35 - Running average train loss is 0.012356000528370362 at epoch 26
INFO - 10/17/22 07:03:05 - 0:08:35 - Average dev loss is 0.016022969046249412 at epoch 26
INFO - 10/17/22 07:03:06 - 0:08:36 - dev accuracy is 91.5468 at epoch 26
INFO - 10/17/22 07:03:06 - 0:08:36 - dev phenome error rate is 0.0097 at epoch 26
INFO - 10/17/22 07:03:06 - 0:08:36 - At 27-th epoch with lr 0.000016.
INFO - 10/17/22 07:03:23 - 0:08:54 - Running average train loss is 0.013446168908547074 at epoch 27
INFO - 10/17/22 07:03:23 - 0:08:54 - Average dev loss is 0.014262971985870256 at epoch 27
INFO - 10/17/22 07:03:25 - 0:08:55 - dev accuracy is 92.8058 at epoch 27
INFO - 10/17/22 07:03:25 - 0:08:55 - dev phenome error rate is 0.0088 at epoch 27
INFO - 10/17/22 07:03:25 - 0:08:55 - At 28-th epoch with lr 0.000016.
INFO - 10/17/22 07:03:42 - 0:09:13 - Running average train loss is 0.01272039454499795 at epoch 28
INFO - 10/17/22 07:03:42 - 0:09:13 - Average dev loss is 0.014231154705003763 at epoch 28
INFO - 10/17/22 07:03:43 - 0:09:14 - dev accuracy is 92.7158 at epoch 28
INFO - 10/17/22 07:03:43 - 0:09:14 - dev phenome error rate is 0.0088 at epoch 28
INFO - 10/17/22 07:03:44 - 0:09:14 - At 29-th epoch with lr 0.000016.
INFO - 10/17/22 07:04:01 - 0:09:32 - Running average train loss is 0.012793607933254599 at epoch 29
INFO - 10/17/22 07:04:01 - 0:09:32 - Average dev loss is 0.01381444100003976 at epoch 29
INFO - 10/17/22 07:04:02 - 0:09:33 - dev accuracy is 92.8957 at epoch 29
INFO - 10/17/22 07:04:02 - 0:09:33 - dev phenome error rate is 0.0087 at epoch 29
INFO - 10/17/22 07:04:02 - 0:09:33 - At 30-th epoch with lr 0.000016.
INFO - 10/17/22 07:04:20 - 0:09:51 - Running average train loss is 0.01293598784886263 at epoch 30
INFO - 10/17/22 07:04:20 - 0:09:51 - Average dev loss is 0.013943048900602242 at epoch 30
INFO - 10/17/22 07:04:21 - 0:09:52 - dev accuracy is 92.6259 at epoch 30
INFO - 10/17/22 07:04:21 - 0:09:52 - dev phenome error rate is 0.0089 at epoch 30
INFO - 10/17/22 07:04:21 - 0:09:52 - At 31-th epoch with lr 0.000010.
INFO - 10/17/22 07:04:39 - 0:10:09 - Running average train loss is 0.013222461463858328 at epoch 31
INFO - 10/17/22 07:04:39 - 0:10:10 - Average dev loss is 0.012694556587554801 at epoch 31
INFO - 10/17/22 07:04:40 - 0:10:11 - dev accuracy is 93.705 at epoch 31
INFO - 10/17/22 07:04:40 - 0:10:11 - dev phenome error rate is 0.0074 at epoch 31
INFO - 10/17/22 07:04:40 - 0:10:11 - At 32-th epoch with lr 0.000010.
INFO - 10/17/22 07:04:58 - 0:10:28 - Running average train loss is 0.013246082080387075 at epoch 32
INFO - 10/17/22 07:04:58 - 0:10:29 - Average dev loss is 0.012715676056149487 at epoch 32
INFO - 10/17/22 07:04:59 - 0:10:30 - dev accuracy is 93.5252 at epoch 32
INFO - 10/17/22 07:04:59 - 0:10:30 - dev phenome error rate is 0.0081 at epoch 32
INFO - 10/17/22 07:04:59 - 0:10:30 - Early stopping triggered with epoch 32 (previous dev loss: 0.012695, current: 0.012716)
INFO - 10/17/22 07:04:59 - 0:10:30 - loading model/test/hardattention/small/transformer0.2/ndebele.nll_0.0127.acc_93.705.per_0.0074.epoch_31 for testing
INFO - 10/17/22 07:04:59 - 0:10:30 - load model in model/test/hardattention/small/transformer0.2/ndebele.nll_0.0127.acc_93.705.per_0.0074.epoch_31
INFO - 10/17/22 07:05:00 - 0:10:31 - Average dev loss is 0.012694556587554801 at epoch -1
INFO - 10/17/22 07:05:00 - 0:10:31 - decoding dev set
INFO - 10/17/22 07:05:02 - 0:10:32 - finished decoding 1292 dev instance
INFO - 10/17/22 07:05:02 - 0:10:32 - DEV accuracy is 93.705 at epoch -1
INFO - 10/17/22 07:05:02 - 0:10:32 - DEV phenome error rate is 0.0074 at epoch -1
INFO - 10/17/22 07:05:02 - 0:10:32 - DEV ndebele acc 93.705 per 0.0074
INFO - 10/17/22 07:05:03 - 0:10:33 - Average test loss is 0.31179128057556227 at epoch -1
INFO - 10/17/22 07:05:03 - 0:10:33 - decoding test set
INFO - 10/17/22 07:05:06 - 0:10:37 - finished decoding 2552 test instance
INFO - 10/17/22 07:05:06 - 0:10:37 - TEST accuracy is 52.4572 at epoch -1
INFO - 10/17/22 07:05:06 - 0:10:37 - TEST phenome error rate is 0.1077 at epoch -1
INFO - 10/17/22 07:05:06 - 0:10:37 - TEST ndebele acc 52.4572 per 0.1077
